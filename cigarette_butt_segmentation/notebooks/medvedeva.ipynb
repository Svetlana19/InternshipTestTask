{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегментация изображений применяется для анализа медицинских изображений (например, определение границ раковых опухолей), в беспилотных автомобилях (например, для навигации на дороге и выявлении пешеходов), видеонаблюдения, дополненнной реальности и решения многих других задач. Одними из первых алгоритмов сегментации изображений были histogram-based bundling [1], thresholding (перевод изображения в градациях серого к бинарному) [2], метод k-средних [3], watersheds[4], active contour model [5] и ряд других методов. Однако в последнее время в связи с активным развитием методов глубокого обучения стали успешно развиваться методы сегментации изображения на основе сетей глубокого обучения. \n",
    "\n",
    "В общих чертах задача сегментации изображений сводится к задаче классификации пикселей изображения по выбранным меткам/классам (semantic segmentation и instance segmentation). Среди методов сегментации изображений на основе глубоких сетей могут быть выделены следующие группы[6]:\n",
    "\n",
    "1. Fully convolutional networks[7];\n",
    "\n",
    "2. Convolutional models with graphical models[8];\n",
    "\n",
    "3. Encoder-decoder based models[9];\n",
    "\n",
    "4. Multi-scale and pyramid network based models[10];\n",
    "\n",
    "5. R-CNN based models (for instance segmentation)[11];\n",
    "\n",
    "6. Dilated convolutional models and DeepLab family[12];\n",
    "\n",
    "7. Recurrent neural network based models[13];\n",
    "\n",
    "8. Attention-based models[14];\n",
    "\n",
    "9. Generative models and adversarial training[15];\n",
    "\n",
    "10. Convolutional models with active contour model[16];\n",
    "\n",
    "\n",
    "Одними из наиболее часто используемых моделей являются U-Net[17] и Mask R-CNN[18].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. G. Thomas. Image segmentation using histogram specification. 15th IEEE International Conference on Image Processing, 2008.\n",
    "\n",
    "2. N. Otsu, “A threshold selection method from gray-level histograms,” IEEE transactions on systems, man, and cybernetics, vol. 9, no. 1, pp. 62–66, 1979.\n",
    "\n",
    "3. N. Dhanachandra, K. Manglem, and Y. J. Chanu, “Image segmentation using k-means clustering algorithm and subtractive clustering algorithm,” Procedia Computer Science, vol. 54, pp. 764–771, 2015.\n",
    "\n",
    "4. L. Najman and M. Schmitt, “Watershed of a continuous function,” Signal Processing, vol. 38, no. 1, pp. 99–112, 1994.\n",
    "\n",
    "5. M. Kass, A. Witkin, and D. Terzopoulos, “Snakes: Active contour models,” International journal of computer vision, vol. 1, no. 4, pp. 321–331, 1988.\n",
    "    \n",
    "6. S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, D. Terzopoulos. Image Segmentation Using Deep Learning: A Survey. arXiv:2001.05566v2\n",
    "        \n",
    "7. J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431–3440.\n",
    "\n",
    "8. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic image segmentation with deep convolutional nets and fully connected crfs,” arXiv preprint arXiv:1412.7062, 2014.\n",
    "\n",
    "9. H. Noh, S. Hong, and B. Han, “Learning deconvolution network for semantic segmentation,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1520–1528.\n",
    "\n",
    "10. T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proceedings of the IEEE conference on computer vision and pattern\n",
    "recognition, 2017, pp. 2117–2125.\n",
    "\n",
    "11. K. He, G. Gkioxari, P. Doll´ar, and R. Girshick, “Mask r-cnn,” Proceedings of the IEEE international conference on computer vision, 2017, pp. 2961–2969.\n",
    "\n",
    "12. L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic image segmentation with deep convolutional nets and fully connected crfs,” arXiv preprint arXiv:1412.7062, 2014.\n",
    "\n",
    "13. F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho, Y. Bengio, M. Matteucci, and A. Courville, “Reseg: A recurrent neural network-based model for semantic segmentation,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2016, pp. 41–48.\n",
    "    \n",
    "14. L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale: Scale-aware semantic image segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3640–3649.\n",
    "    \n",
    "15. P. Luc, C. Couprie, S. Chintala, and J. Verbeek, “Semantic segmentation using adversarial networks,” arXiv preprint arXiv:1611.08408, 2016.    \n",
    "    \n",
    "16. T. H. N. Le, K. G. Quach, K. Luu, C. N. Duong, and M. Savvides, “Reformulating level sets as deep recurrent neural network approach to semantic segmentation,” IEEE Transactions on Image\n",
    "Processing, vol. 27, no. 5, pp. 2393–2407, 2018.\n",
    "\n",
    "17. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597v1.\n",
    "        \n",
    "18. Kaiming He Georgia Gkioxari Piotr Doll´ar Ross Girshick. Mask R-CNN, arXiv:1703.06870v3.         \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных зависит от постановки задачи и исходных данных. \n",
    "Однако наиболее часто предобработка состоит из усреднения изображения с целью подавления шумов, исключение из изображения несущественных по размеру деталей и центрирование объектов на изображении. Исходя из того что оптимальной является линейная функция изменения интенсивности пикселей изображения, а изображения часто являются малоконтрастными, то линеаризация яркости изображения позволяет улучшить его качество.\n",
    "\n",
    "В случае небольшого набора данных можно дополнить исходные изображения их трансформациями, а именно изображениями полученными при повороте, сдвиге по вертикале/горизонтале, приближении/удалении, комбинации нескольких изображений (можно реализовать с помощью ImageDataGenerator), перевороты изображения, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "За основу можно взять модель уже применявшуюся для решения такого рода задач (в случае необходимости поменять число слоёв) и подобрать параметры оптимальным образом:\n",
    "- выбрать наиболее подходящий оптимизатор (SGD, Adam, AdamW, Adamax, SparseAdam, ASGD, LBFGS, RMSprop, Rprop и др.), используя сначала рекомендуемые параметры оптимизатора;\n",
    "- выбрать подходящую активационную функцию (ReLu, сигмоида, гиперболический тангенс, softmax и др.);\n",
    "- подобрать скорость обучения;\n",
    "- подходящую инициализацию весов (kaiming, xavier);\n",
    "- проследить будет ли наблюдаться переобучение модели;\n",
    "- определить число эпох обучения достаточных для получения требуемой точности предсказаний модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob # модуль для работы с путями\n",
    "\n",
    "from lib import *\n",
    "# встраивание изображений в тетрадку\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание маски\n",
    "def get_masks(img_dir, img_id, img_anns, coco):\n",
    "    \"\"\"\n",
    "    Creates image masks for people in the image. mask_all contains all masked people, mask_miss contains\n",
    "    only the masks for people without keypoints.\n",
    "    \"\"\"\n",
    "    img_path = os.path.join(img_dir, \"%012d.jpg\" % img_id)\n",
    "    img = cv2.imread(img_path)\n",
    "    h, w, c = img.shape\n",
    "\n",
    "    mask_all = np.zeros((h, w), dtype=np.uint8)\n",
    "    mask_miss = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    flag = 0\n",
    "    for p in img_anns:\n",
    "        if p[\"iscrowd\"] == 1:\n",
    "            mask_crowd = coco.annToMask(p)\n",
    "            temp = np.bitwise_and(mask_all, mask_crowd)\n",
    "            mask_crowd = mask_crowd - temp\n",
    "            flag += 1\n",
    "            continue\n",
    "        else:\n",
    "            mask = coco.annToMask(p)\n",
    "\n",
    "        mask_all = np.bitwise_or(mask, mask_all)\n",
    "        if p[\"num_keypoints\"] <= 0:\n",
    "            mask_miss = np.bitwise_or(mask, mask_miss)\n",
    "\n",
    "    if flag < 1:\n",
    "        mask_miss = np.logical_not(mask_miss)\n",
    "    elif flag == 1:\n",
    "        mask_miss = np.logical_not(np.bitwise_or(mask_miss, mask_crowd))\n",
    "        mask_all = np.bitwise_or(mask_all, mask_crowd)\n",
    "    else:\n",
    "        raise Exception(\"crowd segments > 1\")\n",
    "\n",
    "    mask_miss = mask_miss.astype(np.uint8)\n",
    "    mask_miss *= 255\n",
    "\n",
    "    mask_all = mask_all.astype(np.uint8)\n",
    "    mask_all *= 255\n",
    "\n",
    "    return img, mask_miss, mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пописывание пути к данным и загрузка изображений\n",
    "path = \"C:/Disk_D/machine_learning/mipt_machine_learning/InternshipTestTask-master/cig_butts/train\"\n",
    "images = os.listdir(f\"{path}/images\")\n",
    "annotations = json.load(open(f\"{path}/coco_annotations.json\", \"r\"))\n",
    "img_id = int(np.random.choice(images).split(\".\")[0])\n",
    "\n",
    "img = np.array(Image.open(f\"{path}/images/{img_id:08}.jpg\"))\n",
    "mask = get_mask(img_id, annotations)\n",
    "show_img_with_mask(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модель U-net (из https://www.kaggle.com/dhananjay3/image-segmentation-from-scratch-in-pytorch)\n",
    "class double_conv(nn.Module):\n",
    "    \"\"\"(conv => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    # описание слоёв нейронной сети\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1), # 1-й свёрточный слой\n",
    "            nn.BatchNorm2d(out_ch), # нормализация\n",
    "            nn.ReLU(inplace=True), # добавление нелинейности с помощью функции активации\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1), # 2-й свёрточный слой\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256, False)\n",
    "        self.up2 = up(512, 128, False)\n",
    "        self.up3 = up(256, 64, False)\n",
    "        self.up4 = up(128, 64, False)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# настройка параметров обучения\n",
    "criterion = BCEDiceLoss(eps=1.0, activation=None) #\n",
    "optimizer = RAdam(model.parameters(), lr = 0.005) # задаём оптимизатор (в данной случае rectified Adam, но можно попробовать любой другой) и скорость скорость обучения\n",
    "current_lr = [param_group['lr'] for param_group in optimizer.param_groups][0]\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, cooldown=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "n_epochs = 32 # число эпох обучения\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "dice_score_list = []\n",
    "lr_rate_list = []\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    dice_score = 0.0\n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    bar = tq(train_loader, postfix={\"train_loss\":0.0})\n",
    "    for data, target in bar:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        #print(loss)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item()})\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    del data, target\n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_loader, postfix={\"valid_loss\":0.0, \"dice_score\":0.0})\n",
    "        for data, target in bar:\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            if train_on_gpu:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, target)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "            dice_cof = dice_no_threshold(output.cpu(), target.cpu()).item()\n",
    "            dice_score +=  dice_cof * data.size(0)\n",
    "            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"dice_score\":dice_cof})\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    dice_score = dice_score/len(valid_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    dice_score_list.append(dice_score)\n",
    "    lr_rate_list.append([param_group['lr'] for param_group in optimizer.param_groups])\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {}  Training Loss: {:.6f}  Validation Loss: {:.6f} Dice Score: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss, dice_score))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "    \n",
    "    scheduler.step(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# построение метрик\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in lr_rate_list])\n",
    "plt.ylabel('learing rate during training', fontsize=22)\n",
    "plt.show()\n",
    "\n",
    "# построение loss\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_loss_list,  marker='o', label=\"Training Loss\")\n",
    "plt.plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\n",
    "plt.ylabel('loss', fontsize=22)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# метрика Dice\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(dice_score_list)\n",
    "plt.ylabel('Dice score')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
